# 神经网络学习

## 非线性假设
当特征 $n$ 很多的时候，多项式拟合便不再适用。
图像数据是高维度的矩阵，因此 使用 非线性假设 会事半功倍。
> $50 \times 50$ 像素的图像 具有 $2500$ 个像素，如果是 $RGB$ 彩色图像，特征会有 $7500$ 个
> 
> 图像的特征 表示为 $x=\begin{bmatrix}pixel \ 1 \ intensity\\ pixel \ 2 \ intensity\\ \vdots \\ pixel \ 2500 \ intensity \end{bmatrix}$ 
> 
> intensity 取值 在 $ intensity \in [0,255]$

## 神经元 与 大脑

### 神经网络
神经元为大脑神经网络的最基本组成单元，我们可以抽象这个结构模型。
``` mermaid
graph LR
    A((x0)) --> E((神经元))
    B((x1)) --> E
    C((x2)) --> E
    D((x3)) --> E
    E --> F(( 输出 ))
```
> $x_0$ 偏差单元 一般可以省略；
> 输出函数 : $h_\theta(x) = \dfrac{1}{1+e^{-\theta^Tx}}$
> Sigmoid ( logistic ) activation function :  $g(z) = \dfrac{1}{1+e^{-z}}$

多层神经网络
* 第一层 称为 输入层 (Input Layer)
* 最后一层 称为 输出层 (Output Layer)
* 其余 层 称为 隐藏层 (Hidden Layer)
* $a_i^{(j)}$ 第 $j$ 层 的第 $i$ 个活动的神经单元
* $\Theta^{(j)}$ 第 $j$ 层的权重

> 如果 神经网络在第 $j$ 层有 $s_j$ 个单元；同理 神经网络在第 $j+1$ 层有 $s_{j+1}$ 个单元; 权重矩阵的维度为 $s_{j+1} \times (s_j + 1)$

## 前向传播

$a_1^{(2)} = g(\Theta^{(1)}_{10} x_0+\Theta^{(1)}_{11} x_1+\Theta^{(1)}_{12} x_2+\Theta^{(1)}_{13} x_3)$ 
$a_2^{(2)} = g(\Theta^{(1)}_{20} x_0+\Theta^{(1)}_{21} x_1+\Theta^{(1)}_{22} x_2+\Theta^{(1)}_{23} x_3)$
$a_3^{(2)} = g(\Theta^{(1)}_{30} x_0+\Theta^{(1)}_{31} x_1+\Theta^{(1)}_{32} x_2+\Theta^{(1)}_{33} x_3)$

$h_{\Theta}(x) = g(\Theta^{(2)}_{10} a_0+\Theta^{(2)}_{11} a_1+\Theta^{(2)}_{12} a_2+\Theta^{(2)}_{13} a_3)$

> 其中定义 $z_1^{(2)} = \Theta^{(1)}_{10} x_0+\Theta^{(1)}_{11} x_1+\Theta^{(1)}_{12} x_2+\Theta^{(1)}_{13} x_3$ 
> 
> $x = \begin{bmatrix}x_0\\x_1\\x_2\\x_3\end{bmatrix}$ , $z^{(2)} = \begin{bmatrix}z_1^{(2)}\\z_2^{(2)}\\z_3^{(2)}\end{bmatrix} = \Theta^{(1)}x$ , $a^{(2)} = g(z^{(2)}) = g(\Theta^{(1)}x)$

### 例子
* AND 与运算  $h_\Theta(x) = g(-30+20*x_1+20*x_2)$
    | $x_1$ | $x_2$ |   $h_\Theta(x)$    |
    | :---: | :---: | :----------------: |
    |   0   |   0   | $g(-30) \approx 0$ |
    |   0   |   1   | $g(-10) \approx 0$ |
    |   1   |   0   | $g(-10) \approx 0$ |
    |   1   |   1   | $g(10) \approx 1$  |

* OR 或运算  $h_\Theta(x) = g(-10+20*x_1+20*x_2)$
    | $x_1$ | $x_2$ |   $h_\Theta(x)$    |
    | :---: | :---: | :----------------: |
    |   0   |   0   | $g(-10) \approx 0$ |
    |   0   |   1   | $g(10) \approx 1$  |
    |   1   |   0   | $g(10) \approx 1$  |
    |   1   |   1   | $g(30) \approx 1$  |

* NOT 取反运算  $h_\Theta(x) = g(10-20*x_1)$
    | $x_1$ |   $h_\Theta(x)$    |
    | :---: | :----------------: |
    |   0   | $g(10) \approx 1$  |
    |   1   | $g(-10) \approx 0$ |

* XNOR 异或运算 
  * AND : $a_1^{(2)}$  $x_1$ AND $x_2$ 
    > $a_1^{(2)} = g(-30+20*x_1+20*x_2)$
  * NOR : $a_2^{(2)}$  ( NOT $x_1$ ) AND ( NOT $x_2$ )
    > $a_2^{(2)} = g(10-20*x_1-20*x_2)$
  * OR  : $h_\Theta(x)$  $x_1$ OR $x_2$
    > $h_\Theta(x) = g(-10+20*a_1^{(2)}+20*a_2^{(2)})$

    | $x_1$ | $x_2$ | $a_1^{(2)}$ | $a_2^{(2)}$ | $h_\Theta(x)$ |
    | :---: | :---: | :---------: | :---------: | :-----------: |
    |   0   |   0   |      0      |      1      |       1       |
    |   0   |   1   |      0      |      0      |       0       |
    |   1   |   0   |      0      |      0      |       0       |
    |   1   |   1   |      1      |      0      |       1       |

* XOR 异或
  * AND : $a_1^{(2)}$  $x_1$ AND $x_2$ 
    > $a_1^{(2)} = g(-30+20*x_1+20*x_2)$
  * NOR : $a_2^{(2)}$  ( NOT $x_1$ ) AND ( NOT $x_2$ )
    > $a_2^{(2)} = g(10-20*x_1-20*x_2)$
  * NOR  : $h_\Theta(x)$  ( NOT $x_1$ ) AND ( NOT $x_2$ ) == NOT ( $x_1$ OR $x_2$ )
    > $h_\Theta(x) = g(10-20*a_1^{(2)}-20*a_2^{(2)})$

    | $x_1$ | $x_2$ | $a_1^{(2)}$ | $a_2^{(2)}$ | $h_\Theta(x)$ |
    | :---: | :---: | :---------: | :---------: | :-----------: |
    |   0   |   0   |      0      |      1      |       0       |
    |   0   |   1   |      0      |      0      |       1       |
    |   1   |   0   |      0      |      0      |       1       |
    |   1   |   1   |      1      |      0      |       0       |

## 多元分类
模型最后的输出是多个结果，例如 $h_\Theta(x) \in \Re^4$
* 训练集: $(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})$
  
* 输出 : $y^{(i)} \in \Biggl\{ \begin{bmatrix}1\\0\\0\\0\end{bmatrix},\begin{bmatrix}0\\1\\0\\0\end{bmatrix},\begin{bmatrix}0\\0\\1\\0\end{bmatrix},\begin{bmatrix}0\\0\\0\\1\end{bmatrix} \Biggr\}$ 

> 最后输出会采用一些方法 输出 得分最高的类别，以及其分数

## 反向传播

